# coding=utf-8
import pandas as pd
import keras
from keras.models import Model
from keras.layers import Input, Dense, Dropout, BatchNormalization, Conv1D, MaxPooling1D, AveragePooling1D, concatenate, \
    Activation, ZeroPadding1D
from keras.layers import add, Flatten
# from keras.layers.convolutional import Conv2D,MaxPooling2D,AveragePooling2D
import numpy as np
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
import os
from keras.callbacks import ModelCheckpoint

# os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["CUDA_VISIBLE_DEVICES"] = "1"


def Conv1d_BN(x, nb_filter, kernel_size, strides=1, padding='same', name=None):
    if name is not None:
        bn_name = name + '_bn'
        conv_name = name + '_conv'
    else:
        bn_name = None
        conv_name = None
    x = Conv1D(nb_filter, kernel_size, padding=padding, strides=strides, activation='relu', name=conv_name)(x)
    x = BatchNormalization(axis=2, name=bn_name)(x)
    return x


def Conv_Block(inpt, nb_filter, kernel_size, strides=1, with_conv_shortcut=False):
    x = Conv1d_BN(inpt, nb_filter=nb_filter, kernel_size=kernel_size, strides=strides, padding='same')
    x = Conv1d_BN(x, nb_filter=nb_filter, kernel_size=kernel_size, padding='same')
    if with_conv_shortcut:
        shortcut = Conv1d_BN(inpt, nb_filter=nb_filter, strides=strides, kernel_size=kernel_size)
        x = add([x, shortcut])
        return x
    else:
        x = add([x, inpt])
        return x


# inpt = Input(shape=(81,1))
# x = ZeroPadding1D(1)(inpt)
# x = Conv1d_BN(x, nb_filter=64, kernel_size=7, strides=2, padding='valid')

def data_net():
    data1 = pd.read_csv('/home/msmal/object_type/train_scale.csv', header=0)
    print("data1 read over!")
    data2 = pd.read_csv('/home/msmal/float_type/train.csv', header=0)
    data2[np.isinf(data2)] = -1
    data2[np.isnan(data2)] = -2
    print("data2 read over!")
    y = pd.read_csv('/home/msmal/label.csv', header=0)
    scaler = preprocessing.StandardScaler()
    data2 = scaler.fit_transform(data2)
    X_scaled = pd.concat([data1.iloc[:, 1:], pd.DataFrame(data2)], axis=1)
    encoder = LabelEncoder()
    y = encoder.fit_transform(y)
    dimension = data1.shape[1] + data2.shape[1] - 1
    X_scaled = X_scaled.values
    X_train = X_scaled.reshape(len(y), dimension)
    X_train = X_train.reshape(X_train.shape[0], dimension, 1).astype('float32')
    num_classes = len(set(y))
    Y_train = y.reshape(y.shape[0], 1).astype('float32')
    Y_train2 = keras.utils.to_categorical(Y_train, num_classes).astype('float32')

    inpt = Input(shape=X_train.shape[1:])
    x = Conv1d_BN(inpt, nb_filter=32, kernel_size=3, strides=2, padding='valid')
    x = MaxPooling1D(pool_size=3, strides=1, padding='same')(x)
    # (56,56,64)
    x = Conv_Block(x, nb_filter=32, kernel_size=3)
    x = Conv_Block(x, nb_filter=32, kernel_size=3)
    # x = Conv_Block(x, nb_filter=64, kernel_size=3)
    # (28,28,128)
    x = Conv_Block(x, nb_filter=64, kernel_size=3, strides=2, with_conv_shortcut=True)
    x = Conv_Block(x, nb_filter=64, kernel_size=3)
    x = Conv_Block(x, nb_filter=64, kernel_size=3)
    # x = Conv_Block(x, nb_filter=128, kernel_size=3)
    # (14,14,256)
    x = Conv_Block(x, nb_filter=128, kernel_size=3, strides=2, with_conv_shortcut=True)
    x = Conv_Block(x, nb_filter=128, kernel_size=3)
    x = Conv_Block(x, nb_filter=128, kernel_size=3)
    # x = Conv_Block(x, nb_filter=256, kernel_size=3)
    # x = Conv_Block(x, nb_filter=256, kernel_size=3)
    # x = Conv_Block(x, nb_filter=256, kernel_size=3)
    # (7,7,512)
    # x = Conv_Block(x, nb_filter=512, kernel_size=3, strides=2, with_conv_shortcut=True)
    # x = Conv_Block(x, nb_filter=512, kernel_size=3)
    # x = Conv_Block(x, nb_filter=512, kernel_size=3)
    x = AveragePooling1D(pool_size=3)(x)
    x = Flatten()(x)
    x = Dropout(0.3)(x)
    x = Dense(2, activation='softmax')(x)
    model = Model(inputs=inpt, outputs=x)
    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])

    filepath = "weights-improvement-{epoch:02d}-{acc:.2f}.hdf5"
    # 中途训练效果提升, 则将文件保存, 每提升一次, 保存一次
    checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True,period=20,
                                 mode='max')
    callback_list=[checkpoint]
    model.summary()

    batch = 25600
    epoch = 150
    model.fit(X_train, Y_train2, batch_size=batch, epochs=epoch, callbacks=callback_list)
    model.save('resnet.h5')


def test_model():
    data1 = pd.read_csv('/home/msmal/object_type/test_scale.csv', header=0)
    print("data1 read over!")
    data2 = pd.read_csv('/home/msmal/float_type/test_scale.csv', header=0)
    print("data2 read over!")
    x = pd.concat([data1.iloc[:, 1:], data2], axis=1)
    dimension = data1.shape[1] + data2.shape[1] - 1
    x = x.values
    x_test = x.reshape(data1.shape[0], dimension)
    x_test = x_test.reshape(x_test.shape[0], dimension, 1).astype('float32')

    from keras.models import load_model
    model=load_model('weight_acc.hdf5')
    res=model.predict(x_test)
    res = np.argmax(res, axis=1)
    res = pd.DataFrame(res)
    sample = pd.read_csv('/home/msmal/sample_submission.csv', header=0)
    sam_index = sample.iloc[:, 0]
    submit = pd.concat([pd.DataFrame(sam_index), res], axis=1)
    submit.to_csv('19-1-27submit.csv', header=sample.columns, index=False)